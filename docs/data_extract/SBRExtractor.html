<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />
    <meta name="generator" content="pdoc 0.10.0" />
    <title>data_extract.SBRExtractor API documentation</title>
    <link rel="icon" type="image/x-icon" href="../sfyr_logo.png" />
    <meta name="description" content="" />
    <link
      rel="preload stylesheet"
      as="style"
      href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css"
      integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs="
      crossorigin
    />
    <link
      rel="preload stylesheet"
      as="style"
      href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css"
      integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg="
      crossorigin
    />
    <link
      rel="stylesheet preload"
      as="style"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css"
      crossorigin
    />
    <style>
      :root {
        --highlight-color: #fe9;
      }
      .flex {
        display: flex !important;
      }
      body {
        line-height: 1.5em;
      }
      #content {
        padding: 20px;
      }
      #sidebar {
        padding: 30px;
        overflow: hidden;
      }
      #sidebar > *:last-child {
        margin-bottom: 2cm;
      }
      .http-server-breadcrumbs {
        font-size: 130%;
        margin: 0 0 15px 0;
      }
      #footer {
        font-size: 0.75em;
        padding: 5px 30px;
        border-top: 1px solid #ddd;
        text-align: right;
      }
      #footer p {
        margin: 0 0 0 1em;
        display: inline-block;
      }
      #footer p:last-child {
        margin-right: 30px;
      }
      h1,
      h2,
      h3,
      h4,
      h5 {
        font-weight: 300;
      }
      h1 {
        font-size: 2.5em;
        line-height: 1.1em;
      }
      h2 {
        font-size: 1.75em;
        margin: 1em 0 0.5em 0;
      }
      h3 {
        font-size: 1.4em;
        margin: 25px 0 10px 0;
      }
      h4 {
        margin: 0;
        font-size: 105%;
      }
      h1:target,
      h2:target,
      h3:target,
      h4:target,
      h5:target,
      h6:target {
        background: var(--highlight-color);
        padding: 0.2em 0;
      }
      a {
        color: #058;
        text-decoration: none;
        transition: color 0.3s ease-in-out;
      }
      a:hover {
        color: #e82;
      }
      .title code {
        font-weight: bold;
      }
      h2[id^="header-"] {
        margin-top: 2em;
      }
      .ident {
        color: #900;
      }
      pre code {
        background: #f8f8f8;
        font-size: 0.8em;
        line-height: 1.4em;
      }
      code {
        background: #f2f2f1;
        padding: 1px 4px;
        overflow-wrap: break-word;
      }
      h1 code {
        background: transparent;
      }
      pre {
        background: #f8f8f8;
        border: 0;
        border-top: 1px solid #ccc;
        border-bottom: 1px solid #ccc;
        margin: 1em 0;
        padding: 1ex;
      }
      #http-server-module-list {
        display: flex;
        flex-flow: column;
      }
      #http-server-module-list div {
        display: flex;
      }
      #http-server-module-list dt {
        min-width: 10%;
      }
      #http-server-module-list p {
        margin-top: 0;
      }
      .toc ul,
      #index {
        list-style-type: none;
        margin: 0;
        padding: 0;
      }
      #index code {
        background: transparent;
      }
      #index h3 {
        border-bottom: 1px solid #ddd;
      }
      #index ul {
        padding: 0;
      }
      #index h4 {
        margin-top: 0.6em;
        font-weight: bold;
      }
      @media (min-width: 200ex) {
        #index .two-column {
          column-count: 2;
        }
      }
      @media (min-width: 300ex) {
        #index .two-column {
          column-count: 3;
        }
      }
      dl {
        margin-bottom: 2em;
      }
      dl dl:last-child {
        margin-bottom: 4em;
      }
      dd {
        margin: 0 0 1em 3em;
      }
      #header-classes + dl > dd {
        margin-bottom: 3em;
      }
      dd dd {
        margin-left: 2em;
      }
      dd p {
        margin: 10px 0;
      }
      .name {
        background: #eee;
        font-weight: bold;
        font-size: 0.85em;
        padding: 5px 10px;
        display: inline-block;
        min-width: 40%;
      }
      .name:hover {
        background: #e0e0e0;
      }
      dt:target .name {
        background: var(--highlight-color);
      }
      .name > span:first-child {
        white-space: nowrap;
      }
      .name.class > span:nth-child(2) {
        margin-left: 0.4em;
      }
      .inherited {
        color: #999;
        border-left: 5px solid #eee;
        padding-left: 1em;
      }
      .inheritance em {
        font-style: normal;
        font-weight: bold;
      }
      .desc h2 {
        font-weight: 400;
        font-size: 1.25em;
      }
      .desc h3 {
        font-size: 1em;
      }
      .desc dt code {
        background: inherit;
      }
      .source summary,
      .git-link-div {
        color: #666;
        text-align: right;
        font-weight: 400;
        font-size: 0.8em;
        text-transform: uppercase;
      }
      .source summary > * {
        white-space: nowrap;
        cursor: pointer;
      }
      .git-link {
        color: inherit;
        margin-left: 1em;
      }
      .source pre {
        max-height: 500px;
        overflow: auto;
        margin: 0;
      }
      .source pre code {
        font-size: 12px;
        overflow: visible;
      }
      .hlist {
        list-style: none;
      }
      .hlist li {
        display: inline;
      }
      .hlist li:after {
        content: ",\2002";
      }
      .hlist li:last-child:after {
        content: none;
      }
      .hlist .hlist {
        display: inline;
        padding-left: 1em;
      }
      img {
        max-width: 100%;
      }
      td {
        padding: 0 0.5em;
      }
      .admonition {
        padding: 0.1em 0.5em;
        margin-bottom: 1em;
      }
      .admonition-title {
        font-weight: bold;
      }
      .admonition.note,
      .admonition.info,
      .admonition.important {
        background: #aef;
      }
      .admonition.todo,
      .admonition.versionadded,
      .admonition.tip,
      .admonition.hint {
        background: #dfd;
      }
      .admonition.warning,
      .admonition.versionchanged,
      .admonition.deprecated {
        background: #fd4;
      }
      .admonition.error,
      .admonition.danger,
      .admonition.caution {
        background: lightpink;
      }
    </style>
    <style media="screen and (min-width: 700px)">
      @media screen and (min-width: 700px) {
        #sidebar {
          width: 30%;
          height: 100vh;
          overflow: auto;
          position: sticky;
          top: 0;
        }
        #content {
          width: 70%;
          max-width: 100ch;
          padding: 3em 4em;
          border-left: 1px solid #ddd;
        }
        pre code {
          font-size: 1em;
        }
        .item .name {
          font-size: 1em;
        }
        main {
          display: flex;
          flex-direction: row-reverse;
          justify-content: flex-end;
        }
        .toc ul ul,
        #index ul {
          padding-left: 1.5em;
        }
        .toc > ul > li {
          margin-top: 0.5em;
        }
      }
    </style>
    <style media="print">
      @media print {
        #sidebar h1 {
          page-break-before: always;
        }
        .source {
          display: none;
        }
      }
      @media print {
        * {
          background: transparent !important;
          color: #000 !important;
          box-shadow: none !important;
          text-shadow: none !important;
        }
        a[href]:after {
          content: " (" attr(href) ")";
          font-size: 90%;
        }
        a[href][title]:after {
          content: none;
        }
        abbr[title]:after {
          content: " (" attr(title) ")";
        }
        .ir a:after,
        a[href^="javascript:"]:after,
        a[href^="#"]:after {
          content: "";
        }
        pre,
        blockquote {
          border: 1px solid #999;
          page-break-inside: avoid;
        }
        thead {
          display: table-header-group;
        }
        tr,
        img {
          page-break-inside: avoid;
        }
        img {
          max-width: 100% !important;
        }
        @page {
          margin: 0.5cm;
        }
        p,
        h2,
        h3 {
          orphans: 3;
          widows: 3;
        }
        h1,
        h2,
        h3,
        h4,
        h5,
        h6 {
          page-break-after: avoid;
        }
      }
    </style>
    <script
      defer
      src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"
      integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8="
      crossorigin
    ></script>
    <script>
      window.addEventListener("DOMContentLoaded", () =>
        hljs.initHighlighting()
      );
    </script>
  </head>
  <body>
    <main>
      <article id="content">
        <header>
          <h1 class="title">Module <code>data_extract.SBRExtractor</code></h1>
        </header>
        <section id="section-intro">
          <details class="source">
            <summary>
              <span>Expand source code</span>
            </summary>
            <pre><code class="python">from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
import pandas as pd
import json
import sys
from datetime import datetime, timedelta
from dateutil import parser


class SBRExtractor:
    def __init__(self):
        with open(&#39;utils/serviceAccount.json&#39;, &#39;r&#39;) as jsonFile:
            self.cred = json.load(jsonFile)
        self.url = self.cred[&#34;dataSources&#34;][&#34;sbr_url&#34;]
        self.req = None
        self.SBR_data_store = pd.DataFrame(
            columns=[&#39;Title&#39;, &#39;Text&#39;, &#39;Link&#39;, &#39;Date&#39;])
        self.start_date = None
        self.end_date = None
        print(&#34;INFO: SBRExtractor initialised&#34;)

    def noOfPages(self):
        &#34;&#34;&#34;Obtains the number of pages from the website

        Returns:
            integer: Number of Pages
        &#34;&#34;&#34;
        self.req = Request(self.url, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
        webpage = urlopen(self.req).read()
        soup = BeautifulSoup(webpage, &#34;lxml&#34;)
        noOfPages = soup.find(&#39;a&#39;, attrs={&#39;title&#39;: &#39;Go to last page&#39;})[
            &#39;href&#39;][6:]
        return int(noOfPages)

    def scrapeURL(self, url):
        &#34;&#34;&#34;This function scrapes the data from the provided URL

        Args:
            url (string): The URL to be scrapped

        Returns:
            dictionary: Output from the URL
        &#34;&#34;&#34;
        self.req = Request(url, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
        webpage = urlopen(self.req).read()

        soup = BeautifulSoup(webpage, &#34;lxml&#34;)
        metadata = soup.find(&#39;article&#39;)

        output_dict = dict()

        # Title
        title = metadata.find(&#39;h1&#39;, attrs={&#39;class&#39;: &#39;nf__title&#39;}).text.strip()
        output_dict[&#39;title&#39;] = title

        # Text
        text = metadata.find(
            &#39;div&#39;, attrs={&#39;class&#39;: &#39;nf__description&#39;}).get_text(strip=True)
        clean_text = text.replace(u&#39;\xa0&#39;, u&#39; &#39;)
        output_dict[&#39;text&#39;] = clean_text

        # Date
        script = soup.find(
            &#39;script&#39;, attrs={&#39;type&#39;: &#39;application/ld+json&#39;}).text
        dictionary = json.loads(script)
        date = dictionary[&#39;@graph&#39;][0][&#39;datePublished&#39;]
        output_dict[&#39;date&#39;] = parser.parse(date)

        return output_dict

    def extract_SBR_data(self, start_date=None, end_date=None):
        &#34;&#34;&#34;This function extracts data from SBR

        Args:
            start_date (datetime, optional): Earliest date of article publish to be scrapped. Defaults to None.
            end_date (datetime, optional): Latest date of article publish to be scrapped. Defaults to None.

        Raises:
            Exception: Sart date input must be before end date input

        Returns:
            boolean: Success/Failure of Extraction
        &#34;&#34;&#34;
        print(&#34;INFO: Extracting SBR Data ...&#34;)

        # If start date is None, then temporarily sets start date as 2001-01-01 as SBR was founded in 2001
        # (All articles will be scrapped)
        self.start_date = start_date if (
            start_date is not None) else datetime(2001, 1, 1)

        # If end date is None, then end date will be current datetime
        self.end_date = end_date if (
            end_date is not None) else datetime.now()

        self.end_date = self.end_date + timedelta(days=1)

        if (self.start_date is not None and self.end_date is not None and self.start_date &gt; self.end_date):
            raise Exception(
                f&#39;ERROR: Start date {self.start_date} input must be before end date {self.end_date} input&#39;)

        noOfPages = self.noOfPages()

        for page in range(0, noOfPages+1):
            print(
                f&#34;&gt;&gt; ========== Extracting SBR Overall Progress: {page}/{noOfPages}&#34;)
            url_page = self.url + &#39;?page=&#39; + str(page)
            self.req = Request(url_page, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
            webpage = urlopen(self.req).read()

            soup = BeautifulSoup(webpage, &#34;lxml&#34;)
            soup = soup.find(&#39;main&#39;, attrs={&#39;class&#39;: &#39;main-content&#39;})
            links = soup.find_all(
                &#39;div&#39;, attrs={&#39;class&#39;: &#39;item with-border-bottom&#39;})

            for j in links:
                link = j.find(
                    &#34;h2&#34;, attrs={&#39;class&#39;: &#39;item__title size-24&#39;}).find(&#39;a&#39;)[&#39;href&#39;].strip()
                output_dict = self.scrapeURL(link)

                article_date = output_dict[&#39;date&#39;].replace(tzinfo=None)
                if article_date &gt; self.end_date:
                    continue

                if article_date &gt;= self.start_date:
                    sys.stdout.write(
                        &#34;INFO: Currently scrapping article of datetime: %s \r&#34; % (article_date))
                    sys.stdout.flush()
                    self.SBR_data_store = self.SBR_data_store.append(
                        {&#39;Link&#39;: link, &#39;Title&#39;: output_dict[&#39;title&#39;], &#39;Text&#39;: output_dict[&#39;text&#39;], &#39;Date&#39;: output_dict[&#39;date&#39;]}, ignore_index=True)
                else:
                    print(&#34;SUCCESS: All articles until end date has been scraped&#34;)
                    print(&#34;SUCCESS: SBR Data successfully extracted and populated&#34;)
                    return True

    def SBR_data_to_csv(self):
        &#34;&#34;&#34;This function exports SBR data to CSV
        &#34;&#34;&#34;
        self.SBR_data_store.to_csv(
            &#39;.\csv_store\SBR_data_stocks.csv&#39;, index=False, encoding=&#39;utf-8-sig&#39;)
        print(&#34;SUCCESS: SBR Data successfully saved to CSV&#34;)

    def load_SBR_data_from_source(self, start_date=None, end_date=None):
        &#34;&#34;&#34;This function returns SBR data within a timeframe

        Args:
            start_date (datetime, optional): Start date of extraction. Defaults to None.
            end_date (datetime, optional): End date of extraction. Defaults to None.

        Returns:
            dataframe: Output of all the articles from SBR
        &#34;&#34;&#34;
        self.extract_SBR_data(start_date, end_date)
        return self.SBR_data_store</code></pre>
          </details>
        </section>
        <section></section>
        <section></section>
        <section></section>
        <section>
          <h2 class="section-title" id="header-classes">Classes</h2>
          <dl>
            <dt id="data_extract.SBRExtractor.SBRExtractor">
              <code class="flex name class">
                <span>class <span class="ident">SBRExtractor</span></span>
              </code>
            </dt>
            <dd>
              <div class="desc"></div>
              <details class="source">
                <summary>
                  <span>Expand source code</span>
                </summary>
                <pre><code class="python">class SBRExtractor:
    def __init__(self):
        with open(&#39;utils/serviceAccount.json&#39;, &#39;r&#39;) as jsonFile:
            self.cred = json.load(jsonFile)
        self.url = self.cred[&#34;dataSources&#34;][&#34;sbr_url&#34;]
        self.req = None
        self.SBR_data_store = pd.DataFrame(
            columns=[&#39;Title&#39;, &#39;Text&#39;, &#39;Link&#39;, &#39;Date&#39;])
        self.start_date = None
        self.end_date = None
        print(&#34;INFO: SBRExtractor initialised&#34;)

    def noOfPages(self):
        &#34;&#34;&#34;Obtains the number of pages from the website

        Returns:
            integer: Number of Pages
        &#34;&#34;&#34;
        self.req = Request(self.url, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
        webpage = urlopen(self.req).read()
        soup = BeautifulSoup(webpage, &#34;lxml&#34;)
        noOfPages = soup.find(&#39;a&#39;, attrs={&#39;title&#39;: &#39;Go to last page&#39;})[
            &#39;href&#39;][6:]
        return int(noOfPages)

    def scrapeURL(self, url):
        &#34;&#34;&#34;This function scrapes the data from the provided URL

        Args:
            url (string): The URL to be scrapped

        Returns:
            dictionary: Output from the URL
        &#34;&#34;&#34;
        self.req = Request(url, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
        webpage = urlopen(self.req).read()

        soup = BeautifulSoup(webpage, &#34;lxml&#34;)
        metadata = soup.find(&#39;article&#39;)

        output_dict = dict()

        # Title
        title = metadata.find(&#39;h1&#39;, attrs={&#39;class&#39;: &#39;nf__title&#39;}).text.strip()
        output_dict[&#39;title&#39;] = title

        # Text
        text = metadata.find(
            &#39;div&#39;, attrs={&#39;class&#39;: &#39;nf__description&#39;}).get_text(strip=True)
        clean_text = text.replace(u&#39;\xa0&#39;, u&#39; &#39;)
        output_dict[&#39;text&#39;] = clean_text

        # Date
        script = soup.find(
            &#39;script&#39;, attrs={&#39;type&#39;: &#39;application/ld+json&#39;}).text
        dictionary = json.loads(script)
        date = dictionary[&#39;@graph&#39;][0][&#39;datePublished&#39;]
        output_dict[&#39;date&#39;] = parser.parse(date)

        return output_dict

    def extract_SBR_data(self, start_date=None, end_date=None):
        &#34;&#34;&#34;This function extracts data from SBR

        Args:
            start_date (datetime, optional): Earliest date of article publish to be scrapped. Defaults to None.
            end_date (datetime, optional): Latest date of article publish to be scrapped. Defaults to None.

        Raises:
            Exception: Sart date input must be before end date input

        Returns:
            boolean: Success/Failure of Extraction
        &#34;&#34;&#34;
        print(&#34;INFO: Extracting SBR Data ...&#34;)

        # If start date is None, then temporarily sets start date as 2001-01-01 as SBR was founded in 2001
        # (All articles will be scrapped)
        self.start_date = start_date if (
            start_date is not None) else datetime(2001, 1, 1)

        # If end date is None, then end date will be current datetime
        self.end_date = end_date if (
            end_date is not None) else datetime.now()

        self.end_date = self.end_date + timedelta(days=1)

        if (self.start_date is not None and self.end_date is not None and self.start_date &gt; self.end_date):
            raise Exception(
                f&#39;ERROR: Start date {self.start_date} input must be before end date {self.end_date} input&#39;)

        noOfPages = self.noOfPages()

        for page in range(0, noOfPages+1):
            print(
                f&#34;&gt;&gt; ========== Extracting SBR Overall Progress: {page}/{noOfPages}&#34;)
            url_page = self.url + &#39;?page=&#39; + str(page)
            self.req = Request(url_page, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
            webpage = urlopen(self.req).read()

            soup = BeautifulSoup(webpage, &#34;lxml&#34;)
            soup = soup.find(&#39;main&#39;, attrs={&#39;class&#39;: &#39;main-content&#39;})
            links = soup.find_all(
                &#39;div&#39;, attrs={&#39;class&#39;: &#39;item with-border-bottom&#39;})

            for j in links:
                link = j.find(
                    &#34;h2&#34;, attrs={&#39;class&#39;: &#39;item__title size-24&#39;}).find(&#39;a&#39;)[&#39;href&#39;].strip()
                output_dict = self.scrapeURL(link)

                article_date = output_dict[&#39;date&#39;].replace(tzinfo=None)
                if article_date &gt; self.end_date:
                    continue

                if article_date &gt;= self.start_date:
                    sys.stdout.write(
                        &#34;INFO: Currently scrapping article of datetime: %s \r&#34; % (article_date))
                    sys.stdout.flush()
                    self.SBR_data_store = self.SBR_data_store.append(
                        {&#39;Link&#39;: link, &#39;Title&#39;: output_dict[&#39;title&#39;], &#39;Text&#39;: output_dict[&#39;text&#39;], &#39;Date&#39;: output_dict[&#39;date&#39;]}, ignore_index=True)
                else:
                    print(&#34;SUCCESS: All articles until end date has been scraped&#34;)
                    print(&#34;SUCCESS: SBR Data successfully extracted and populated&#34;)
                    return True

    def SBR_data_to_csv(self):
        &#34;&#34;&#34;This function exports SBR data to CSV
        &#34;&#34;&#34;
        self.SBR_data_store.to_csv(
            &#39;.\csv_store\SBR_data_stocks.csv&#39;, index=False, encoding=&#39;utf-8-sig&#39;)
        print(&#34;SUCCESS: SBR Data successfully saved to CSV&#34;)

    def load_SBR_data_from_source(self, start_date=None, end_date=None):
        &#34;&#34;&#34;This function returns SBR data within a timeframe

        Args:
            start_date (datetime, optional): Start date of extraction. Defaults to None.
            end_date (datetime, optional): End date of extraction. Defaults to None.

        Returns:
            dataframe: Output of all the articles from SBR
        &#34;&#34;&#34;
        self.extract_SBR_data(start_date, end_date)
        return self.SBR_data_store</code></pre>
              </details>
              <h3>Methods</h3>
              <dl>
                <dt id="data_extract.SBRExtractor.SBRExtractor.SBR_data_to_csv">
                  <code class="name flex">
                    <span>def <span class="ident">SBR_data_to_csv</span></span
                    >(<span>self)</span>
                  </code>
                </dt>
                <dd>
                  <div class="desc">
                    <p>This function exports SBR data to CSV</p>
                  </div>
                  <details class="source">
                    <summary>
                      <span>Expand source code</span>
                    </summary>
                    <pre><code class="python">def SBR_data_to_csv(self):
    &#34;&#34;&#34;This function exports SBR data to CSV
    &#34;&#34;&#34;
    self.SBR_data_store.to_csv(
        &#39;.\csv_store\SBR_data_stocks.csv&#39;, index=False, encoding=&#39;utf-8-sig&#39;)
    print(&#34;SUCCESS: SBR Data successfully saved to CSV&#34;)</code></pre>
                  </details>
                </dd>
                <dt
                  id="data_extract.SBRExtractor.SBRExtractor.extract_SBR_data"
                >
                  <code class="name flex">
                    <span>def <span class="ident">extract_SBR_data</span></span
                    >(<span>self, start_date=None, end_date=None)</span>
                  </code>
                </dt>
                <dd>
                  <div class="desc">
                    <p>This function extracts data from SBR</p>
                    <h2 id="args">Args</h2>
                    <dl>
                      <dt>
                        <strong><code>start_date</code></strong>
                        :&ensp;<code>datetime</code>, optional
                      </dt>
                      <dd>
                        Earliest date of article publish to be scrapped.
                        Defaults to None.
                      </dd>
                      <dt>
                        <strong><code>end_date</code></strong>
                        :&ensp;<code>datetime</code>, optional
                      </dt>
                      <dd>
                        Latest date of article publish to be scrapped. Defaults
                        to None.
                      </dd>
                    </dl>
                    <h2 id="raises">Raises</h2>
                    <dl>
                      <dt><code>Exception</code></dt>
                      <dd>Sart date input must be before end date input</dd>
                    </dl>
                    <h2 id="returns">Returns</h2>
                    <dl>
                      <dt><code>boolean</code></dt>
                      <dd>Success/Failure of Extraction</dd>
                    </dl>
                  </div>
                  <details class="source">
                    <summary>
                      <span>Expand source code</span>
                    </summary>
                    <pre><code class="python">def extract_SBR_data(self, start_date=None, end_date=None):
    &#34;&#34;&#34;This function extracts data from SBR

    Args:
        start_date (datetime, optional): Earliest date of article publish to be scrapped. Defaults to None.
        end_date (datetime, optional): Latest date of article publish to be scrapped. Defaults to None.

    Raises:
        Exception: Sart date input must be before end date input

    Returns:
        boolean: Success/Failure of Extraction
    &#34;&#34;&#34;
    print(&#34;INFO: Extracting SBR Data ...&#34;)

    # If start date is None, then temporarily sets start date as 2001-01-01 as SBR was founded in 2001
    # (All articles will be scrapped)
    self.start_date = start_date if (
        start_date is not None) else datetime(2001, 1, 1)

    # If end date is None, then end date will be current datetime
    self.end_date = end_date if (
        end_date is not None) else datetime.now()

    self.end_date = self.end_date + timedelta(days=1)

    if (self.start_date is not None and self.end_date is not None and self.start_date &gt; self.end_date):
        raise Exception(
            f&#39;ERROR: Start date {self.start_date} input must be before end date {self.end_date} input&#39;)

    noOfPages = self.noOfPages()

    for page in range(0, noOfPages+1):
        print(
            f&#34;&gt;&gt; ========== Extracting SBR Overall Progress: {page}/{noOfPages}&#34;)
        url_page = self.url + &#39;?page=&#39; + str(page)
        self.req = Request(url_page, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
        webpage = urlopen(self.req).read()

        soup = BeautifulSoup(webpage, &#34;lxml&#34;)
        soup = soup.find(&#39;main&#39;, attrs={&#39;class&#39;: &#39;main-content&#39;})
        links = soup.find_all(
            &#39;div&#39;, attrs={&#39;class&#39;: &#39;item with-border-bottom&#39;})

        for j in links:
            link = j.find(
                &#34;h2&#34;, attrs={&#39;class&#39;: &#39;item__title size-24&#39;}).find(&#39;a&#39;)[&#39;href&#39;].strip()
            output_dict = self.scrapeURL(link)

            article_date = output_dict[&#39;date&#39;].replace(tzinfo=None)
            if article_date &gt; self.end_date:
                continue

            if article_date &gt;= self.start_date:
                sys.stdout.write(
                    &#34;INFO: Currently scrapping article of datetime: %s \r&#34; % (article_date))
                sys.stdout.flush()
                self.SBR_data_store = self.SBR_data_store.append(
                    {&#39;Link&#39;: link, &#39;Title&#39;: output_dict[&#39;title&#39;], &#39;Text&#39;: output_dict[&#39;text&#39;], &#39;Date&#39;: output_dict[&#39;date&#39;]}, ignore_index=True)
            else:
                print(&#34;SUCCESS: All articles until end date has been scraped&#34;)
                print(&#34;SUCCESS: SBR Data successfully extracted and populated&#34;)
                return True</code></pre>
                  </details>
                </dd>
                <dt
                  id="data_extract.SBRExtractor.SBRExtractor.load_SBR_data_from_source"
                >
                  <code class="name flex">
                    <span
                      >def
                      <span class="ident">load_SBR_data_from_source</span></span
                    >(<span>self, start_date=None, end_date=None)</span>
                  </code>
                </dt>
                <dd>
                  <div class="desc">
                    <p>This function returns SBR data within a timeframe</p>
                    <h2 id="args">Args</h2>
                    <dl>
                      <dt>
                        <strong><code>start_date</code></strong>
                        :&ensp;<code>datetime</code>, optional
                      </dt>
                      <dd>Start date of extraction. Defaults to None.</dd>
                      <dt>
                        <strong><code>end_date</code></strong>
                        :&ensp;<code>datetime</code>, optional
                      </dt>
                      <dd>End date of extraction. Defaults to None.</dd>
                    </dl>
                    <h2 id="returns">Returns</h2>
                    <dl>
                      <dt><code>dataframe</code></dt>
                      <dd>Output of all the articles from SBR</dd>
                    </dl>
                  </div>
                  <details class="source">
                    <summary>
                      <span>Expand source code</span>
                    </summary>
                    <pre><code class="python">def load_SBR_data_from_source(self, start_date=None, end_date=None):
    &#34;&#34;&#34;This function returns SBR data within a timeframe

    Args:
        start_date (datetime, optional): Start date of extraction. Defaults to None.
        end_date (datetime, optional): End date of extraction. Defaults to None.

    Returns:
        dataframe: Output of all the articles from SBR
    &#34;&#34;&#34;
    self.extract_SBR_data(start_date, end_date)
    return self.SBR_data_store</code></pre>
                  </details>
                </dd>
                <dt id="data_extract.SBRExtractor.SBRExtractor.noOfPages">
                  <code class="name flex">
                    <span>def <span class="ident">noOfPages</span></span
                    >(<span>self)</span>
                  </code>
                </dt>
                <dd>
                  <div class="desc">
                    <p>Obtains the number of pages from the website</p>
                    <h2 id="returns">Returns</h2>
                    <dl>
                      <dt><code>integer</code></dt>
                      <dd>Number of Pages</dd>
                    </dl>
                  </div>
                  <details class="source">
                    <summary>
                      <span>Expand source code</span>
                    </summary>
                    <pre><code class="python">def noOfPages(self):
    &#34;&#34;&#34;Obtains the number of pages from the website

    Returns:
        integer: Number of Pages
    &#34;&#34;&#34;
    self.req = Request(self.url, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
    webpage = urlopen(self.req).read()
    soup = BeautifulSoup(webpage, &#34;lxml&#34;)
    noOfPages = soup.find(&#39;a&#39;, attrs={&#39;title&#39;: &#39;Go to last page&#39;})[
        &#39;href&#39;][6:]
    return int(noOfPages)</code></pre>
                  </details>
                </dd>
                <dt id="data_extract.SBRExtractor.SBRExtractor.scrapeURL">
                  <code class="name flex">
                    <span>def <span class="ident">scrapeURL</span></span
                    >(<span>self, url)</span>
                  </code>
                </dt>
                <dd>
                  <div class="desc">
                    <p>This function scrapes the data from the provided URL</p>
                    <h2 id="args">Args</h2>
                    <dl>
                      <dt>
                        <strong><code>url</code></strong> :&ensp;<code
                          >string</code
                        >
                      </dt>
                      <dd>The URL to be scrapped</dd>
                    </dl>
                    <h2 id="returns">Returns</h2>
                    <dl>
                      <dt><code>dictionary</code></dt>
                      <dd>Output from the URL</dd>
                    </dl>
                  </div>
                  <details class="source">
                    <summary>
                      <span>Expand source code</span>
                    </summary>
                    <pre><code class="python">def scrapeURL(self, url):
    &#34;&#34;&#34;This function scrapes the data from the provided URL

    Args:
        url (string): The URL to be scrapped

    Returns:
        dictionary: Output from the URL
    &#34;&#34;&#34;
    self.req = Request(url, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})
    webpage = urlopen(self.req).read()

    soup = BeautifulSoup(webpage, &#34;lxml&#34;)
    metadata = soup.find(&#39;article&#39;)

    output_dict = dict()

    # Title
    title = metadata.find(&#39;h1&#39;, attrs={&#39;class&#39;: &#39;nf__title&#39;}).text.strip()
    output_dict[&#39;title&#39;] = title

    # Text
    text = metadata.find(
        &#39;div&#39;, attrs={&#39;class&#39;: &#39;nf__description&#39;}).get_text(strip=True)
    clean_text = text.replace(u&#39;\xa0&#39;, u&#39; &#39;)
    output_dict[&#39;text&#39;] = clean_text

    # Date
    script = soup.find(
        &#39;script&#39;, attrs={&#39;type&#39;: &#39;application/ld+json&#39;}).text
    dictionary = json.loads(script)
    date = dictionary[&#39;@graph&#39;][0][&#39;datePublished&#39;]
    output_dict[&#39;date&#39;] = parser.parse(date)

    return output_dict</code></pre>
                  </details>
                </dd>
              </dl>
            </dd>
          </dl>
        </section>
      </article>
      <nav id="sidebar">
        <img src="../sfyr_logo.png" , style="width: 100px; height: 100px" />
        <div class="toc">
          <ul></ul>
        </div>
        <ul id="index">
          <li>
            <h3>Super-module</h3>
            <ul>
              <li>
                <code
                  ><a title="Syfr_API" href="../index.html"
                    >Syfr API Documentation</a
                  ></code
                >
              </li>
              <li>
                <code
                  ><a title="data_extract" href="index.html"
                    >data_extract</a
                  ></code
                >
              </li>
            </ul>
          </li>
          <li>
            <h3><a href="#header-classes">Classes</a></h3>
            <ul>
              <li>
                <h4>
                  <code
                    ><a
                      title="data_extract.SBRExtractor.SBRExtractor"
                      href="#data_extract.SBRExtractor.SBRExtractor"
                      >SBRExtractor</a
                    ></code
                  >
                </h4>
                <ul class="">
                  <li>
                    <code
                      ><a
                        title="data_extract.SBRExtractor.SBRExtractor.SBR_data_to_csv"
                        href="#data_extract.SBRExtractor.SBRExtractor.SBR_data_to_csv"
                        >SBR_data_to_csv</a
                      ></code
                    >
                  </li>
                  <li>
                    <code
                      ><a
                        title="data_extract.SBRExtractor.SBRExtractor.extract_SBR_data"
                        href="#data_extract.SBRExtractor.SBRExtractor.extract_SBR_data"
                        >extract_SBR_data</a
                      ></code
                    >
                  </li>
                  <li>
                    <code
                      ><a
                        title="data_extract.SBRExtractor.SBRExtractor.load_SBR_data_from_source"
                        href="#data_extract.SBRExtractor.SBRExtractor.load_SBR_data_from_source"
                        >load_SBR_data_from_source</a
                      ></code
                    >
                  </li>
                  <li>
                    <code
                      ><a
                        title="data_extract.SBRExtractor.SBRExtractor.noOfPages"
                        href="#data_extract.SBRExtractor.SBRExtractor.noOfPages"
                        >noOfPages</a
                      ></code
                    >
                  </li>
                  <li>
                    <code
                      ><a
                        title="data_extract.SBRExtractor.SBRExtractor.scrapeURL"
                        href="#data_extract.SBRExtractor.SBRExtractor.scrapeURL"
                        >scrapeURL</a
                      ></code
                    >
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </nav>
    </main>
    <footer id="footer">
      <p>
        Generated by
        <a
          href="https://pdoc3.github.io/pdoc"
          title="pdoc: Python API documentation generator"
          ><cite>pdoc</cite> 0.10.0</a
        >.
      </p>
    </footer>
  </body>
</html>
